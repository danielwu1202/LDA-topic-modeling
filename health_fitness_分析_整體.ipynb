{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 讀取檔案與前處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('health_fitness.csv', engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "x = ['way', 'product', 'day', 'indiegogo', 'campaign', 'year', 'thing', 'campaigns', 'products', \n",
    "     'days', 'things', 'design', 'designing', 'designs', 'years', 'low', 'world', 'people', 'production',\n",
    "    'project', 'projects', 'problem', 'problems', 'come', 'comes', 'coming', 'first', 'today', 'item', 'items',\n",
    "    'other', 'others', 'possible', 'possibility', 'result', 'quality', 'backer', 'backers', 'amp', 'need',\n",
    "     'class', 'user', 'users', 'parts', 'part', 'friendly', 'datum', 'units', 'unit', 'using'\n",
    "     'time', 'help', 'life', 'health','vapor_soothers', 'programs', 'catspad', 'Remedium', 'Diawater',\n",
    "    'perk', 'perks', 'DIRTEA', 'Stemoscope', 'research', 'remedium',  'dirtea', 'Gentletent', 'gentletent', 'device'\n",
    "    , 'brush effects', 'BRUSH EFFECTS', 'dirtea', 'diawater', 'WalkingPad', 'walkingpad', 'TinyMount', 'order', 'orders',\n",
    "    'market', 'biosband', 'work', 'working', 'works', 'time', 'times', 'timing', 'spoontek', 'tinymount', 'xculpter']\n",
    "for i in x:\n",
    "    stop_words.append(i)\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.description.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "        \n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "print(data_words[:1])\n",
    "print(len(data_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=1, threshold=60) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=50)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "# See trigram example\n",
    "print(trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "char = ['NOUN','VERB','ADJ', 'ADV']\n",
    "\n",
    "def lemmatization(texts, allowed_postags=char):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words)\n",
    "\n",
    "\n",
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words_bigrams)\n",
    "\n",
    "\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load('en', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_nostops, allowed_postags=['NOUN'])\n",
    "\n",
    "# Stemming\n",
    "porter = PorterStemmer()\n",
    "lancaster=LancasterStemmer()\n",
    "# print(data_words[1])\n",
    "\n",
    "data_words_stem = []\n",
    "for num in range(len(data_lemmatized)):\n",
    "    words_stem = []\n",
    "    for word in data_lemmatized[num]:\n",
    "        word_stem = porter.stem(word)\n",
    "        words_stem.append(word_stem)\n",
    "    data_words_stem.append(words_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('原始文字資料')    \n",
    "print(data[1])\n",
    "print('  ')\n",
    "print('原始文字資料的斷詞')\n",
    "print(data_words[1])\n",
    "print('  ')\n",
    "print('form bigrim')\n",
    "print(data_words_bigrams[1])\n",
    "print('  ')\n",
    "print('removal stopwords後的結果')\n",
    "print(data_words_nostops[1])\n",
    "print('  ')\n",
    "print('Lemmatization後的結果')\n",
    "print(data_lemmatized[1])\n",
    "print('  ')\n",
    "print('stemming的結果')\n",
    "print(data_words_stem[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 讀取字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Dictionary\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "id2word = Dictionary.load_from_text(r'C:\\Users\\USER\\Desktop\\論文\\LDA\\test25\\health_fitness_dictionary_25')\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_words_stem\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "\n",
    "from gensim.models import TfidfModel\n",
    "tfidf = TfidfModel(corpus, id2word = id2word)\n",
    "\n",
    "'''\n",
    "# #filter low value words\n",
    "low_value = 0.15\n",
    "\n",
    "for i in range(0, len(corpus)):\n",
    "    bow = corpus[i]\n",
    "    low_value_words = [] #reinitialize to be safe. You can skip this.\n",
    "    low_value_words = [id for id, value in tfidf[bow] if value < low_value]\n",
    "    new_bow = [b for b in bow if b[0] not in low_value_words]\n",
    "\n",
    "#reassign        \n",
    "corpus[i] = new_bow\n",
    "\n",
    "# print(corpus[:])\n",
    "# print(tfidf)\n",
    "'''\n",
    "\n",
    "print(corpus[:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 讀取LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "lda = gensim.models.ldamodel.LdaModel.load('C:/Users/USER/Desktop/論文/LDA/test25/lda_model_ver_hf_25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_topics = lda.show_topics(formatted=False)\n",
    "pprint(lda.print_topics(num_topics = 50, num_words=25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "from wordcloud import WordCloud, STOPWORDS \n",
    "\n",
    "for t in range(lda.num_topics): \n",
    "    plt.figure() \n",
    "    plt.imshow(WordCloud(background_color='white').fit_words(dict(lda.show_topic(t, 200))))\n",
    "    plt.axis(\"off\") \n",
    "    plt.title(\"TopiC#\" + str(t+1)) \n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda, corpus, id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def format_topics_sentences(ldamodel=lda, corpus=corpus, texts=data):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=lda, corpus=corpus, texts=data)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "\n",
    "# Show\n",
    "df_dominant_topic.head(810)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(corpus))\n",
    "for i in range(0, len(corpus)):\n",
    "    print(i, \"get_document_topics\", lda.get_document_topics(corpus[i], minimum_probability=0.3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution = []\n",
    "for i in range(0, len(corpus)):\n",
    "    for d in lda.get_document_topics(corpus[i], minimum_probability=0.0):\n",
    "        distribution.append(d[1])\n",
    "    \n",
    "print(len(distribution))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_campaign_distribution = list(chunks(distribution, 11))\n",
    "print(len(all_campaign_distribution))\n",
    "print(all_campaign_distribution[0])\n",
    "print(all_campaign_distribution[1])\n",
    "print(all_campaign_distribution[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_campaign_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('topic_distribution.csv', all_campaign_distribution, delimiter = ',', fmt = '%s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 主題九分群"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_8_campaign_index = []\n",
    "for i in all_campaign_distribution:\n",
    "    if i[8] >= 0.3:\n",
    "        topic_8_campaign_index.append(all_campaign_distribution.index(i))\n",
    "\n",
    "print(len(topic_8_campaign_index))\n",
    "index_excel = []\n",
    "for i in topic_8_campaign_index:\n",
    "    i += 2\n",
    "    index_excel.append(i)\n",
    "\n",
    "print(len(index_excel))\n",
    "print(index_excel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_8_campaign = []\n",
    "for i in topic_8_campaign_index:\n",
    "    topic_8_campaign.append(all_campaign_distribution[i])\n",
    "\n",
    "print(len(topic_8_campaign))\n",
    "print(topic_8_campaign[0])\n",
    "\n",
    "topic_8_array = np.array(topic_8_campaign)\n",
    "print(type(topic_8_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('topic_9_distribution.csv', topic_8_campaign, delimiter = ',', fmt = '%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "kmeans = KMeans(n_clusters=2, max_iter = 800).fit(topic_8_array)\n",
    "#分群結果\n",
    "kmeans.labels_  \n",
    "# 調高次數\n",
    "# 正規化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import pairwise_distances\n",
    "labels = kmeans.labels_\n",
    "metrics.calinski_harabasz_score(topic_8_array, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_1 = KMeans(n_clusters=3, max_iter = 800).fit(topic_8_array)\n",
    "labels_1 = kmeans_1.labels_\n",
    "metrics.calinski_harabasz_score(topic_8_array, labels_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_2 = KMeans(n_clusters=4, max_iter = 800).fit(topic_8_array)\n",
    "labels_2 = kmeans_2.labels_\n",
    "metrics.calinski_harabasz_score(topic_8_array, labels_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components = 2)\n",
    "campaign_pca = pca.fit(topic_8_array)\n",
    "campaign_pca_1 = pca.transform(topic_8_array)\n",
    "print(\"original shape:   \", topic_8_array.shape)\n",
    "print(\"transformed shape:\", campaign_pca_1.shape)\n",
    "print(type(campaign_pca_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_with_pca = KMeans(n_clusters=4, max_iter = 800).fit(campaign_pca_1)\n",
    "kmeans_with_pca.labels_\n",
    "labels_pca = kmeans_with_pca.labels_\n",
    "metrics.calinski_harabasz_score(topic_8_array, labels_pca )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(campaign_pca_1[:,0],campaign_pca_1[:,1],c=kmeans_with_pca.predict(campaign_pca_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "class KMeans:\n",
    "    def cal_dist(self, p0, p1):\n",
    "        \"\"\"\n",
    "        比較兩點的距離\n",
    "        \"\"\"\n",
    "        return np.sqrt(np.sum((p0-p1)**2))\n",
    "\n",
    "    def kmeans(self, datapoints, k=2):\n",
    "        # 定義資料維度\n",
    "        d = datapoints.shape[1]\n",
    "        # 最大的迭代次數\n",
    "        Max_Iterations = 1000\n",
    "\n",
    "        cluster = np.zeros(datapoints.shape[0])\n",
    "        prev_cluster = np.ones(datapoints.shape[0])\n",
    "\n",
    "        cluster_centers = []\n",
    "        for i in range(k):\n",
    "            cluster_centers += [random.choice(datapoints)]\n",
    "\n",
    "        iteration = 0\n",
    "        while np.array_equal(cluster, prev_cluster) is False or iteration > Max_Iterations:\n",
    "            iteration += 1\n",
    "            prev_cluster = cluster.copy()\n",
    "\n",
    "            # 將每一個點做分群\n",
    "            for idx, point in enumerate(datapoints):\n",
    "                min_dist = float(\"inf\")\n",
    "                for c, cluster_center in enumerate(cluster_centers):\n",
    "                    dist = self.cal_dist(point, cluster_center)\n",
    "                    if dist < min_dist:\n",
    "                        min_dist = dist  \n",
    "                        cluster[idx] = c   # 指定該點屬於哪個分群\n",
    "\n",
    "            # 更新分群的中心\n",
    "            for k in range(len(cluster_centers)):\n",
    "                new_center = np.zeros(d)\n",
    "                members = 0\n",
    "                for point, c in zip(datapoints, cluster):\n",
    "                    if c == k:\n",
    "                        new_center += point\n",
    "                        members += 1\n",
    "                if members > 0:\n",
    "                    new_center = new_center / members\n",
    "                cluster_centers[k] = new_center\n",
    "\n",
    "        return cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "K = KMeans()\n",
    "topic_8_cluster_result = K.kmeans(topic_8_array, k)\n",
    "cluster = [[] for _ in range(k)]\n",
    "\n",
    "for idx, c in enumerate(topic_8_cluster_result):\n",
    "    cluster[int(c)].append(topic_8_campaign_index[idx])\n",
    "    \n",
    "for c, result in enumerate(cluster):\n",
    "    print('Cluster {}: {}'.format(c, result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 全部計畫主題統計"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 總主題統計\n",
    "count_all = []\n",
    "for i in range(0, len(corpus)):\n",
    "    for j in lda.get_document_topics(corpus[i], minimum_probability=0.3):\n",
    "            count_all.append(j[0])\n",
    "            \n",
    "print(count_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_count_all = dict((a+1, count_all.count(a)) for a in count_all)\n",
    "print(topic_count_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_all = pd.DataFrame(list(topic_count_all.items()),columns = ['主題','計畫數'])\n",
    "df_topic_all.sort_values(by=['主題'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pylab import mpl\n",
    "mpl.rcParams[\"font.sans-serif\"] = [\"MingLiU\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "x = df_topic_all['主題']\n",
    "y = df_topic_all['計畫數']\n",
    "plt.xlabel('主題編號')\n",
    "plt.ylabel('計畫數')\n",
    "width = 0.35\n",
    "plt.bar(x, y, width)\n",
    "# plt.savefig('整體分布長條圖.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2015計畫主題統計"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_2015 = []\n",
    "for i in range(0, 266):\n",
    "    for j in lda.get_document_topics(corpus[i], minimum_probability=0.3):\n",
    "            count_2015.append(j[0])\n",
    "            \n",
    "print(len(count_2015))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_count_2015 = dict((a+1, count_2015.count(a)) for a in count_2015)\n",
    "print(topic_count_2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf2015 = df_dominant_topic[:266]\n",
    "hf2015.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(hf2015))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_2015 = pd.DataFrame(list(topic_count_2015.items()),columns = ['主題','計畫數'])\n",
    "df_topic_2015.sort_values(by=['主題'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2016計畫主題統計"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_2016 = []\n",
    "for i in range(266, 395):\n",
    "    for j in lda.get_document_topics(corpus[i], minimum_probability=0.3):\n",
    "            count_2016.append(j[0])\n",
    "            \n",
    "print(count_2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_count_2016 = dict((a+1, count_2016.count(a)) for a in count_2016)\n",
    "print(topic_count_2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf2016 = df_dominant_topic[266:395]\n",
    "hf2016.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(hf2016))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_2016 = pd.DataFrame(list(topic_count_2016.items()),columns = ['主題','計畫數'])\n",
    "df_topic_2016.sort_values(by=['主題'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2017計畫主題統計"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_2017 = []\n",
    "for i in range(395, 466):\n",
    "    for j in lda.get_document_topics(corpus[i], minimum_probability=0.3):\n",
    "            count_2017.append(j[0])\n",
    "            \n",
    "print(count_2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_count_2017 = dict((a+1, count_2017.count(a)) for a in count_2017)\n",
    "print(topic_count_2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf2017 = df_dominant_topic[395:466]\n",
    "hf2017.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(hf2017))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_2017 = pd.DataFrame(list(topic_count_2017.items()),columns = ['主題','計畫數'])\n",
    "df_topic_2017.sort_values(by=['主題'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2018計畫主題統計"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_2018 = []\n",
    "for i in range(466, 529):\n",
    "    for j in lda.get_document_topics(corpus[i], minimum_probability=0.3):\n",
    "            count_2018.append(j[0])\n",
    "            \n",
    "print(count_2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_count_2018 = dict((a+1, count_2018.count(a)) for a in count_2018)\n",
    "print(topic_count_2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf2018 = df_dominant_topic[466:529]\n",
    "hf2018.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(hf2018))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_2018 = pd.DataFrame(list(topic_count_2018.items()),columns = ['主題','計畫數'])\n",
    "df_topic_2018.sort_values(by=['主題'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2019計畫主題統計"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_2019 = []\n",
    "for i in range(529, 597):\n",
    "    for j in lda.get_document_topics(corpus[i], minimum_probability=0.3):\n",
    "            count_2019.append(j[0])\n",
    "            \n",
    "print(count_2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_count_2019 = dict((a+1, count_2019.count(a)) for a in count_2019)\n",
    "print(topic_count_2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf2019 = df_dominant_topic[529:597]\n",
    "hf2019.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(hf2019))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_2019 = pd.DataFrame(list(topic_count_2019.items()),columns = ['主題','計畫數'])\n",
    "df_topic_2019.sort_values(by=['主題'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2020計畫主題統計"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_2020 = []\n",
    "for i in range(597, 806):\n",
    "    for j in lda.get_document_topics(corpus[i], minimum_probability=0.3):\n",
    "            count_2020.append(j[0])\n",
    "            \n",
    "print(count_2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_count_2020 = dict((a+1, count_2020.count(a)) for a in count_2020)\n",
    "print(topic_count_2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf2020 = df_dominant_topic[597:]\n",
    "hf2020.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(hf2020))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_2020 = pd.DataFrame(list(topic_count_2020.items()),columns = ['主題','計畫數'])\n",
    "df_topic_2020.sort_values(by=['主題'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 搜尋方式 (精確....)\n",
    "# 其他搜尋引擎 (google、google scholar、新聞......)\n",
    "# 多個來源"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
